{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFORMATION PLANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture # <-- Using sklearn for GMMs\n",
    "from scipy.special import logsumexp # For GMM entropy calculation (though score_samples is used)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Synthetic Data Generation ---\n",
    "def generate_synthetic_data(num_samples, slope, intercept, noise_std, classification_threshold):\n",
    "    \"\"\"\n",
    "    Generates synthetic data for a binary classification task.\n",
    "    Input X: Uniform(0, 1)\n",
    "    Continuous Y: slope * X + intercept + Gaussian_noise\n",
    "    Binary Y: 1 if Continuous Y > classification_threshold, else 0\n",
    "    \"\"\"\n",
    "    X = np.random.uniform(0, 1, num_samples)\n",
    "    noise = np.random.normal(0, noise_std, num_samples)\n",
    "    Y_continuous = slope * X + intercept + noise\n",
    "    Y_binary = (Y_continuous > classification_threshold).astype(int)\n",
    "\n",
    "    # Reshape for PyTorch compatibility (add feature dimension)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
    "    Y_tensor = torch.tensor(Y_binary, dtype=torch.float32).unsqueeze(1)\n",
    "    return X_tensor, Y_tensor, X, Y_binary # Return numpy arrays for MI estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Neural Network Architecture ---\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        # Store activations *after* ReLU for Information Plane\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.relu_layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer to first hidden layer\n",
    "        self.linear_layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        self.relu_layers.append(nn.ReLU())\n",
    "\n",
    "        # Hidden layers. Specifying the dimensions for each layer.\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            self.relu_layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer (no ReLU before sigmoid)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.activations = {} # To store activations for information plane\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store input x for I(X;T)\n",
    "        self.activations['input'] = x.detach().cpu().numpy()\n",
    "        current_activation = x\n",
    "        for i, (linear_layer, relu_layer) in enumerate(zip(self.linear_layers, self.relu_layers)):\n",
    "            current_activation = linear_layer(current_activation)\n",
    "            current_activation = relu_layer(current_activation)\n",
    "            # Store the activation after ReLU\n",
    "            self.activations[f'hidden_{i}'] = current_activation.detach().cpu().numpy()\n",
    "\n",
    "        output = self.output_layer(current_activation)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate_mi_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. GMM-based Mutual Information Estimation ---\n",
    "\n",
    "def estimate_mi_gmm(X_samples, T_samples, n_components=5, cov_type='full', random_state=None):\n",
    "    \"\"\"\n",
    "    Estimates Mutual Information I(X; T) using Gaussian Mixture Models.\n",
    "    X_samples (numpy.ndarray): Input data (e.g., X).\n",
    "    T_samples (numpy.ndarray): Layer activations (e.g., T).\n",
    "    Both are treated as continuous variables.\n",
    "    \"\"\"\n",
    "    X_samples = np.asarray(X_samples)\n",
    "    T_samples = np.asarray(T_samples)\n",
    "\n",
    "    # Ensure X_samples and T_samples have at least 2 dimensions for GMM if they are 1D\n",
    "    if X_samples.ndim == 1:\n",
    "        X_samples = X_samples.reshape(-1, 1)\n",
    "    if T_samples.ndim == 1:\n",
    "        T_samples = T_samples.reshape(-1, 1)\n",
    "\n",
    "    # Check for NaN/Inf in data before fitting GMM\n",
    "    if np.isnan(X_samples).any() or np.isinf(X_samples).any() or \\\n",
    "       np.isnan(T_samples).any() or np.isinf(T_samples).any():\n",
    "        print(\"Warning: NaN or Inf values found in input or activation data. Skipping MI calculation.\")\n",
    "        return 0.0\n",
    "\n",
    "    joint_XT_samples = np.hstack((X_samples, T_samples))\n",
    "\n",
    "    # Heuristic for minimum samples per GMM component to avoid errors\n",
    "    min_samples_for_gmm = n_components * (joint_XT_samples.shape[1] + 1)\n",
    "    if joint_XT_samples.shape[0] < min_samples_for_gmm:\n",
    "        # print(f\"Warning: Not enough samples ({joint_XT_samples.shape[0]}) for GMM for joint XT. Need at least {min_samples_for_gmm}. Returning 0.0.\")\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        # Fit GMM on joint (X, T)\n",
    "        gmm_joint_XT = GaussianMixture(n_components=n_components, covariance_type=cov_type,\n",
    "                                       random_state=random_state, tol=1e-3, max_iter=200, n_init=3)\n",
    "        gmm_joint_XT.fit(joint_XT_samples)\n",
    "        H_XT = -gmm_joint_XT.score(joint_XT_samples) / joint_XT_samples.shape[0] # Average negative log-likelihood\n",
    "\n",
    "        # Fit GMM on X\n",
    "        gmm_X = GaussianMixture(n_components=n_components, covariance_type=cov_type,\n",
    "                                random_state=random_state, tol=1e-3, max_iter=200, n_init=3)\n",
    "        gmm_X.fit(X_samples)\n",
    "        H_X = -gmm_X.score(X_samples) / X_samples.shape[0]\n",
    "\n",
    "        # Fit GMM on T\n",
    "        gmm_T = GaussianMixture(n_components=n_components, covariance_type=cov_type,\n",
    "                                random_state=random_state, tol=1e-3, max_iter=200, n_init=3)\n",
    "        gmm_T.fit(T_samples)\n",
    "        H_T = -gmm_T.score(T_samples) / T_samples.shape[0]\n",
    "\n",
    "        # I(X; T) = H(X) + H(T) - H(X, T)\n",
    "        I_XT = H_X + H_T - H_XT\n",
    "        # Mutual information must be non-negative\n",
    "        I_XT = max(0, I_XT)\n",
    "\n",
    "    except ValueError as e:\n",
    "        # print(f\"Error fitting GMM for I(X; T) or computing entropy: {e}. Returning 0.0.\")\n",
    "        return 0.0 # Return 0 if GMM fitting fails (e.g., not enough samples, singular covariance)\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred during I(X;T) calculation: {e}. Returning 0.0.\")\n",
    "        return 0.0\n",
    "\n",
    "    return I_XT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate_mi_discrete_continuous_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mi_discrete_continuous_gmm(discrete_var, continuous_var, n_components=5, cov_type='full', random_state=None):\n",
    "    \"\"\"\n",
    "    Estimates Mutual Information I(Discrete_Var; Continuous_Var) using GMMs.\n",
    "    discrete_var (numpy.ndarray): 1D array of discrete labels (e.g., 0, 1).\n",
    "    continuous_var (numpy.ndarray): Multi-dimensional array of continuous values (activations).\n",
    "    \"\"\"\n",
    "    discrete_var = np.asarray(discrete_var).flatten()\n",
    "    continuous_var = np.asarray(continuous_var)\n",
    "\n",
    "    if continuous_var.ndim == 1:\n",
    "        continuous_var = continuous_var.reshape(-1, 1)\n",
    "\n",
    "    # Check for NaN/Inf in data before fitting GMM\n",
    "    if np.isnan(continuous_var).any() or np.isinf(continuous_var).any():\n",
    "        print(\"Warning: NaN or Inf values found in continuous data. Skipping MI calculation.\")\n",
    "        return 0.0\n",
    "\n",
    "    # H(Y) - Entropy of the discrete variable\n",
    "    unique_classes, counts = np.unique(discrete_var, return_counts=True)\n",
    "    p_y = counts / len(discrete_var)\n",
    "    H_Y = -np.sum(p_y * np.log(p_y + 1e-10)) # Add small epsilon to avoid log(0)\n",
    "\n",
    "    # H(T) - Entropy of the continuous variable (activations)\n",
    "    # Heuristic for minimum samples per GMM component\n",
    "    min_samples_for_gmm_T = n_components * (continuous_var.shape[1] + 1)\n",
    "    if continuous_var.shape[0] < min_samples_for_gmm_T:\n",
    "        # print(f\"Warning: Not enough samples ({continuous_var.shape[0]}) for GMM for H(T). Need at least {min_samples_for_gmm_T}. Returning 0.0 for I(Y;T).\")\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        gmm_T = GaussianMixture(n_components=n_components, covariance_type=cov_type,\n",
    "                                random_state=random_state, tol=1e-3, max_iter=200, n_init=3)\n",
    "        gmm_T.fit(continuous_var)\n",
    "        H_T = -gmm_T.score(continuous_var) / continuous_var.shape[0]\n",
    "    except ValueError as e:\n",
    "        # print(f\"Error fitting GMM for H(T): {e}. Returning 0.0 for I(Y;T).\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred during H(T) calculation: {e}. Returning 0.0.\")\n",
    "        return 0.0\n",
    "\n",
    "    # H(T|Y) = Sum_{y in classes} p(y) * H(T | Y=y)\n",
    "    H_T_given_Y = 0.0\n",
    "    for i, y_val in enumerate(unique_classes):\n",
    "        # Samples of T where Y = y_val\n",
    "        T_given_Y_samples = continuous_var[discrete_var == y_val]\n",
    "\n",
    "        if len(T_given_Y_samples) == 0:\n",
    "            continue # Skip if no samples for this class\n",
    "\n",
    "        # Heuristic for minimum samples per GMM component for conditional GMM\n",
    "        min_samples_for_conditional_gmm = n_components * (T_given_Y_samples.shape[1] + 1)\n",
    "        if len(T_given_Y_samples) < min_samples_for_conditional_gmm:\n",
    "            # print(f\"Warning: Not enough samples ({len(T_given_Y_samples)}) for GMM for class {y_val}. Skipping this class's contribution.\")\n",
    "            continue # Skip if not enough samples for a robust GMM fit\n",
    "\n",
    "        try:\n",
    "            gmm_T_given_Y = GaussianMixture(n_components=n_components, covariance_type=cov_type,\n",
    "                                            random_state=random_state, tol=1e-3, max_iter=200, n_init=3)\n",
    "            gmm_T_given_Y.fit(T_given_Y_samples)\n",
    "\n",
    "            H_T_given_Y_current = -gmm_T_given_Y.score(T_given_Y_samples) / T_given_Y_samples.shape[0]\n",
    "            H_T_given_Y += p_y[i] * H_T_given_Y_current\n",
    "        except ValueError as e:\n",
    "            # print(f\"Error fitting GMM for H(T|Y={y_val}): {e}. Skipping this class's contribution.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # print(f\"An unexpected error occurred during H(T|Y={y_val}) calculation: {e}. Skipping this class's contribution.\")\n",
    "            continue\n",
    "\n",
    "    I_YT = H_T - H_T_given_Y\n",
    "    I_YT = max(0, I_YT) # Mutual information must be non-negative\n",
    "    return I_YT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "9G28kw6I3g4M",
    "outputId": "1c4cc9a0-403e-4c38-9289-0bcb8da4b718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Starting training and information plane tracking (GMM-based MI with sklearn)...\n",
      "Epoch 1/1000, Loss: 0.2244, Test Accuracy: 0.8680\n",
      "Epoch 100/1000, Loss: 0.3142, Test Accuracy: 0.8580\n",
      "Epoch 200/1000, Loss: 0.2698, Test Accuracy: 0.8640\n",
      "Epoch 300/1000, Loss: 0.2845, Test Accuracy: 0.8700\n",
      "Epoch 400/1000, Loss: 0.2175, Test Accuracy: 0.8650\n",
      "Epoch 500/1000, Loss: 0.1650, Test Accuracy: 0.8640\n",
      "Epoch 600/1000, Loss: 0.3657, Test Accuracy: 0.8660\n",
      "Epoch 700/1000, Loss: 0.2876, Test Accuracy: 0.8640\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    num_samples = 5000 # Increased samples for better GMM fitting\n",
    "    input_dim = 1\n",
    "    hidden_dims = [64, 32] # Keep hidden layers smaller for faster GMM fitting\n",
    "    output_dim = 1\n",
    "    learning_rate = 0.01\n",
    "    epochs = 1000\n",
    "    batch_size = 64\n",
    "\n",
    "    # GPU check\n",
    "    # Determine the device to use (MPS if available, otherwise CPU)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # GMM MI parameters\n",
    "    num_gmm_components = 3 # Number of components for each GMM\n",
    "    gmm_covariance_type = 'full' # 'full', 'tied', 'diag', 'spherical'\n",
    "                                # 'full' is most flexible but computationally expensive\n",
    "                                # 'diag' is often a good compromise for higher dimensions\n",
    "    gmm_random_state = 42\n",
    "\n",
    "    # Generate data\n",
    "    X_tensor, Y_tensor, X_np, Y_np_binary = generate_synthetic_data(num_samples, slope=5, intercept=1, noise_std=0.8, classification_threshold=3.5)\n",
    "\n",
    "    # Corrected line: Unpack all 8 values returned by train_test_split\n",
    "    X_train, X_test, Y_train, Y_test, X_np_train, X_np_test, Y_np_binary_train, Y_np_binary_test = train_test_split(\n",
    "        X_tensor, Y_tensor, X_np, Y_np_binary, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize network, loss, and optimizer\n",
    "    model = SimpleNN(input_dim, hidden_dims, output_dim)\n",
    "    model.to(device) \n",
    "    criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Store MI values for plotting\n",
    "    information_plane_data = {f'hidden_{i}': {'IXT': [], 'IYT': []} for i in range(len(hidden_dims))}\n",
    "\n",
    "    print(\"Starting training and information plane tracking (GMM-based MI with sklearn)...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch training\n",
    "        permutation = torch.randperm(X_train.size(0))\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            indices = permutation[i:i + batch_size]\n",
    "            batch_X, batch_Y = X_train[indices].to(device), Y_train[indices].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0 or epoch == 0: # Estimate MI periodically\n",
    "            # Get activations for the entire training set\n",
    "            #_ = model(X_train) # Forward pass to populate model.activations\n",
    "            _ = model(X_train.to(device))\n",
    "\n",
    "            # Estimate Information Plane coordinates for each hidden layer\n",
    "            for i in range(len(hidden_dims)):\n",
    "                layer_name = f'hidden_{i}'\n",
    "                # Ensure activations are numpy arrays\n",
    "                layer_activations = model.activations[layer_name]\n",
    "\n",
    "                # Reshape activations if they are 1D or higher than 2D (e.g., from Conv layers)\n",
    "                if layer_activations.ndim == 1:\n",
    "                    layer_activations = layer_activations.reshape(-1, 1)\n",
    "                elif layer_activations.ndim > 2:\n",
    "                     layer_activations = layer_activations.reshape(layer_activations.shape[0], -1)\n",
    "\n",
    "                # I(X; T) using GMM\n",
    "                IXT = estimate_mi_gmm(X_np_train, layer_activations,\n",
    "                                      n_components=num_gmm_components,\n",
    "                                      cov_type=gmm_covariance_type,\n",
    "                                      random_state=gmm_random_state)\n",
    "\n",
    "                # I(Y; T) using GMM (discrete Y, continuous T)\n",
    "                IYT = estimate_mi_discrete_continuous_gmm(Y_np_binary_train, layer_activations,\n",
    "                                                        n_components=num_gmm_components,\n",
    "                                                        cov_type=gmm_covariance_type,\n",
    "                                                        random_state=gmm_random_state)\n",
    "\n",
    "                information_plane_data[layer_name]['IXT'].append(IXT)\n",
    "                information_plane_data[layer_name]['IYT'].append(IYT)\n",
    "\n",
    "            # Calculate accuracy on test set\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                # Move test data to device for inference\n",
    "                test_outputs = model(X_test.to(device))\n",
    "                Y_test_device = Y_test.to(device)\n",
    "                predicted_classes = (test_outputs > 0.5).float()\n",
    "                accuracy = (predicted_classes == Y_test_device).float().mean().item()\n",
    "            model.train() # Set model back to training mode\n",
    "            \n",
    "\n",
    "            current_loss = loss.item() if 'loss' in locals() else 'N/A'\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {current_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # --- 4. Plotting the Information Plane ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, len(hidden_dims)))\n",
    "\n",
    "    for i, layer_name in enumerate(information_plane_data.keys()):\n",
    "        IXT_values = information_plane_data[layer_name]['IXT']\n",
    "        IYT_values = information_plane_data[layer_name]['IYT']\n",
    "\n",
    "        if len(IXT_values) > 0:\n",
    "            plt.plot(IXT_values, IYT_values, 'o-', color=colors[i], label=f'Layer {i+1} ({hidden_dims[i]} neurons)', alpha=0.7)\n",
    "            # Add annotations for start and end points\n",
    "            if len(IXT_values) > 1:\n",
    "                plt.text(IXT_values[0], IYT_values[0], 'Start', fontsize=8, color=colors[i])\n",
    "                plt.text(IXT_values[-1], IYT_values[-1], 'End', fontsize=8, color=colors[i])\n",
    "            else: # If only one point, just mark it\n",
    "                plt.text(IXT_values[0], IYT_values[0], 'Point', fontsize=8, color=colors[i])\n",
    "\n",
    "    plt.xlabel('$I(X; T)$ (Mutual Information between Input and Layer Activation)')\n",
    "    plt.ylabel('$I(Y; T)$ (Mutual Information between Output and Layer Activation)')\n",
    "    plt.title('Information Plane Trajectories During Training (GMM-based MI with sklearn)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated data are:\n",
    "* X_tensor,\n",
    "* *Y_tensor,\n",
    "* X_np,\n",
    "* Y_np_binary\n",
    "\n",
    "The classification threshold is 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(X_tensor, bins=100)\n",
    "_ = plt.title(\"X Tensor\")\n",
    "plt.figure()\n",
    "_ = plt.hist(Y_tensor, bins=100)\n",
    "_ = plt.title(\"Y Tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
